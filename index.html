<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <style type="text/css">
        body {
          background-color: #ffffff;
          font-family: Palatino;
        }
        .container {
            background-color: #fff;
            zoom: 1;
            margin-left: auto;
            margin-right: auto;
            vertical-align: middle;
            text-align: left;
            width: 100%;
            max-width: 800px;
            padding: 20px;
            margin: 20px auto;
        }
        .content {
            margin-bottom: -10px;
            display: inline-block;
            display*: inline;
            width: 100%;
        }
        a {
          color: #03c;
          text-decoration: none;
          transition: 0.3s all cubic-bezier(0.42, 0, 0.57, 1.96);
        }
    
        a:focus,
        a:hover{
          color: #FF4500;
          border-color: #FF4500;
        }
        .publogo { width: 230px; max-height: 190px; margin-right : 15px; float : left; border : 0;}
        .publication { clear : left; padding-bottom : 0px; }
        .publication p { height : 100px; padding-top : 5px;}
        .publication strong a { font-size: 17px; color : #0000A0; }
        .publication .links { position : relative; top : 15px }
        .publication .links a { margin-right : 20px; }
    </style>
    <title>Jihan Yang's Homepage</title>
    <!-- <link rel="icon" href="../content/images/icon.png"> -->
  </head>
  <body>
    <div class="container">
        <table width="100%" align="center" border="0">
<tr>
    <td width="55%" valign="middle">
        <h1 style="margin-top:18px;margin-bottom:36px;">Jihan Yang</h1>
        <p style="font-size:16px; color:rgb(0, 0, 0)">
	        <b style="color: rgb(0, 0, 0)">Postdoctoral Associate</b> <br />
          <span style="color: rgb(0, 0, 0)">Courant Institute of Mathematical Sciences</span>  <br />
	        <span style="color: rgb(0, 0, 0)">New York University</span> 
            <br /><br />
            Email: jihanyang [at] nyu [dot] edu
        </p>
        <p>
            <a href="https://scholar.google.com/citations?user=zWfNZnIAAAAJ&hl=en">[Google Scholar]</a> &nbsp;
            <a href="https://github.com/jihanyang">[GitHub]</a> &nbsp;
            <a href="https://twitter.com/jihanyang13">[Twitter]</a> &nbsp;
        </p>
    </td>
    <td width="45%" align="center">
        <img src="content/images/myfigure.jpg" width="100%" />
    </td>
</tr>
</table>

<h2 id="about-me" style="color:rgb(86, 1, 141)">Biography</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />

<p>I am a postdoctoral associate at NYU Courant, hosted by <a href="https://www.sainingxie.com/">Prof. Saining Xie</a>. Before that, I received by Ph.D degree from The University of Hong Kong, advised by <a href="https://xjqi.github.io/">Prof. Xiaojuan Qi</a>. Prior to that,
I obtained my Bachelor's degree from Sun Yat-sen University, supervised by <a href="http://www.linliang.net/">Prof. Liang Lin</a> and <a href="http://guanbinli.com/">Prof. Guanbin Li</a>.
</p>

<p>My research interests lie in machine learning and computer vision, with a particular focus on Multimodal Large Language Models on reasoning, unified models, long-context, visual-spatial intelligence, and grounded into real-world applications.</p>

<p style="color:#FF4500"><strong><i>(We are looking for self-motivated interns with strong background in NYU. Please email me if youâ€™re interested!)</i></strong></p>

<h2 id="news" style="color:rgb(86, 1, 141)">News</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />

<ul>
  <li>[2025/05] <span style="color: #FF4500;">UniTok</span> was accepted by NeurIPS 2025 as <span style="color: #FF4500;">spotlight</span>.</li>
  <li>[2025/05] <span style="color: #FF4500;">SFTvsRL</span> was accepted by ICML 2025.</li>
  <li>[2025/03] <span style="color: #FF4500;">Thinking in Space</span> was accepted by CVPR 2025 as <span style="color: #FF4500;">Oral</span>.</li>
  <li>[2024/09] <span style="color: #FF4500;">Cambrian-1</span> was accepted by NeurIPS 2024 as <span style="color: #FF4500;">Oral</span>.</li>
  <li>[2024/07] <span style="color: #FF4500;">V-IRL</span> was accepted by ECCV 2024.</li>
  <li>[2024/05] <span style="color: #FF4500;">Lowis3D</span> was accepted by T-PAMI.</li>
  <li>[2024/02] <span style="color: #FF4500;">RegionPLC</span> was accepted by CVPR 2024.</li>
  <li>[2023/02] <span style="color: #FF4500;">PLA</span> was accepted by CVPR 2023.</li>
  <li>[2022/09] <span style="color: #FF4500;">ST3D++</span> was accepted by T-PAMI.</li>
  <li>[2022/09] <span style="color: #FF4500;">SparseKD</span> was accepted by NeurIPS 2022.</li>
  <li>[2022/07] <span style="color: #FF4500;">DODA</span> was accepted by ECCV 2022.</li>
</ul>

<script type="text/javascript">
    function hideshow(which){
    if (!document.getElementById)
        return
    if (which.style.display=="block")
        which.style.display="none"
    else
        which.style.display="block"
    }
</script>



<h2 id="publications" style="color:rgb(86, 1, 141)">Selected Projects <a href="https://scholar.google.com/citations?user=zWfNZnIAAAAJ&hl=en">[Google Scholar]</a></h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />

<p>*: Equal Contribution</p>

<!-- **Selected Preprints:** -->

<!-- <ul>
  
</ul> -->

<!-- **2024:** -->

<div class="publication">
  <img src="content/images/unitok_teaser.png" class="publogo" style="width: 30%;">
  <p> 
      <strong>
          <a href="https://arxiv.org/abs/2502.20321">UniTok: A Unified Tokenizer for Visual Generation and Understanding
          </a>
      </strong>
      <br>
      Chuofan Ma, Yi Jiang, Junfeng Wu, <strong>Jihan Yang</strong>, Xin Yu, Zehuan Yuan, Bingyue Peng, Xiaojuan Qi
      <br>
      <em>Advances in Neural Information Processing Systems (NeurIPS) 2025.</em>
      <span style="color: #FF4500;"><strong>[Spotlight]</strong></span>
      <br>
      <span class="links">
        <a href="https://arxiv.org/abs/2502.20321">[PDF]</a>
        <a href="https://foundationvision.github.io/UniTok/">[BLOG]</a>
        <a href="https://github.com/FoundationVision/UniTok">[CODE]</a><a href="https://github.com/FoundationVision/UniTok"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/FoundationVision/UniTok?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>
<br>

<br>



<div class="publication">
  <img src="content/images/sftvsrl.jpg" class="publogo" style="width: 30%;">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2501.17161">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training
          </a>
      </strong>
      <br>
      Tianzhe Chu*, Yuexiang Zhai*, <strong>Jihan Yang</strong>, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma
      <br>
      <br>
      <em>International Conference on Machine Learning (ICML), 2025</em>
      <br>
      <span class="links">
        <a href="https://arxiv.org/pdf/2501.17161">[PDF]</a>
        <a href="https://tianzhechu.com/SFTvsRL/">[BLOG]</a>
        <a href="https://github.com/LeslieTrue/SFTvsRL">[CODE]</a><a href="https://github.com/LeslieTrue/SFTvsRL"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LeslieTrue/SFTvsRL?style=social" /> </a>
        <a href="https://huggingface.co/collections/tianzhechu/sftvsrl-models-and-data-6797ba6de522c7de7fcb80ba">[DATA]</a>
    
      </span>
  </p>
</div>
<br>
<br>
<br>
<br>
<br>
<br>

<div class="publication">
  <img src="content/images/think_in_space_teaser.png" class="publogo" style="width: 30%;">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2412.14171.pdf">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces
          </a>
      </strong>
      <br>
      <strong>Jihan Yang*</strong>, Shusheng Yang*, Anjali W. Gupta*, Rilyn Han*, Li Fei-Fei, Saining Xie.
      <br>
      <br>
      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em>
      <span style="color: #FF4500;"><strong>[Oral]</strong></span>
      <br>
      <span class="links">
        <a href="https://arxiv.org/pdf/2412.14171.pdf">[PDF]</a>
        <a href="https://vision-x-nyu.github.io/thinking-in-space.github.io/">[BLOG]</a>
        <a href="https://github.com/vision-x-nyu/thinking-in-space">[CODE]</a><a href="https://github.com/vision-x-nyu/thinking-in-space"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space?style=social" /> </a>
        <a href="https://huggingface.co/datasets/nyu-visionx/VSI-Bench">[BENCH]</a>
    
      </span>
  </p>
</div>
<br>
<br>
<br>
<br>
<br>
<br>


<div class="publication">
  <img src="content/images/cambrian_teaser.png" class="publogo" style="width: 30%;">
  <p> 
      <strong>
          <a href="https://arxiv.org/abs/2406.16860">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs
          </a>
      </strong>
      <br>
      Shengbang Tong*, Ellis Brown*, Penghao Wu*, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, <strong>Jihan Yang</strong>, 
      Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, Saining Xie.
      <br>
      <br>
      <em>Advances in Neural Information Processing Systems (NeurIPS) 2024.</em>
      <span style="color: #FF4500;"><strong>[Oral]</strong></span>
      <br>
      <span class="links">
          <a href="https://arxiv.org/pdf/2406.16860.pdf">[PDF]</a>
          <a href="https://cambrian-mllm.github.io/">[BLOG]</a>
          <a href="https://github.com/cambrian-mllm/cambrian">[CODE]</a><a href="https://github.com/cambrian-mllm/cambrian"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cambrian-mllm/cambrian?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>
<br>
<br>
<br>
<br>


<div class="publication">
  <img src="content/images/VIRL_teaser.png" class="publogo">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2402.03310.pdf"><i>V-IRL</i>: Grounding Virtual Intelligence in Real Life
          </a>
      </strong>
      <br>
      <b>Jihan Yang</b>, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie.
      <br>
      <br>
      <em>European Conference on Computer Vision (ECCV), 2024.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/pdf/2402.03310.pdf">[PDF]</a>
          <a href="https://virl-platform.github.io">[BLOG]</a>
          <a href="https://github.com/VIRL-Platform/VIRL">[CODE]</a><a href="https://github.com/VIRL-Platform/VIRL"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/VIRL-Platform/VIRL?style=social" /> </a>
      </span>
  </p>
</div>
<br>

<div class="publication">
  <img src="content/images/lowis3d_teaser.png" class="publogo">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2308.00353.pdf">Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding</a>
      </strong>
      <br>
      Runyu Ding, <b>Jihan Yang</b>, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi.
      <br>
      <br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) 2024.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/pdf/2308.00353.pdf">[PDF]</a>
      </span>
  </p>
</div>
<br>
<br>
<br>
<br>
<br>



<!-- **2023:** -->
<div class="publication">
  <img src="content/images/RegionPLC.png" class="publogo">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2304.00962.pdf">RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding
          </a>
      </strong>
      <br>
      <b>Jihan Yang*</b>, Runyu Ding*, Weipeng Deng, Zhe Wang, Xiaojuan Qi.
      <br>
      <br>
      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/pdf/2304.00962.pdf">[PDF]</a>
          <a href="https://jihanyang.github.io/projects/RegionPLC">[BLOG]</a>
          <a href="https://github.com/CVMI-Lab/PLA">[CODE]</a><a href="https://github.com/CVMI-Lab/PLA"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/PLA?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>
<br>


<!-- **2023:** -->
<div class="publication">
  <img src="content/images/pla_teaser.gif" class="publogo">
  <p> 
      <strong>
          <a href="https://arxiv.org/pdf/2211.16312.pdf">PLA: Language-driven Open-Vocabulary 3D Scene Understanding</a>
      </strong>
      <br>
      Runyu Ding*, <b>Jihan Yang*</b>, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi.
      <br>
      <br>
      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/pdf/2211.16312.pdf">[PDF]</a>
          <a href="https://dingry.github.io/projects/PLA">[BLOG]</a>
          <a href="https://github.com/CVMI-Lab/PLA">[CODE]</a><a href="https://github.com/CVMI-Lab/PLA"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/PLA?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>

<!-- **2022:** -->
<div class="publication">
  <img src="content/images/sparsekd_teaser.png" class="publogo">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/2205.15156">Towards Efficient 3D Object Detection with Knowledge Distillation</a>
      </strong>
      <br>
      <b>Jihan Yang</b>, Shaoshuai Shi, Runyu Ding, Zhe Wang, Xiaojuan Qi
      <br>
      <br>
      <em>Advances in Neural Information Processing Systems (NeurIPS) 2022.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/2205.15156">[PDF]</a>
          <a href="https://github.com/CVMI-Lab/SparseKD">[CODE]</a> <a href="https://github.com/CVMI-Lab/SparseKD"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/SparseKD?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>
<br>
<br>


<div class="publication">
  <img src="content/images/st3d++_teaser.png" class="publogo">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/2108.06682">ST3D++: Self-training for Unsupervised Domain Adaptation on 3D Object Detection</a>
      </strong>
      <br>
      <b>Jihan Yang</b>, Shaoshuai Shi, Zhe Wang, Hongsheng Li, Xiaojuan Qi
      <br>
      <br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) 2022.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/2108.06682">[PDF]</a>
          <a href="https://github.com/CVMI-Lab/ST3D">[CODE]</a> <a href="https://github.com/CVMI-Lab/ST3D"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/ST3D?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>

<div class="publication">
  <img src="content/images/doda_teaser.png" class="publogo">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/2204.01599">DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Indoor Semantic Segmentation</a>
      </strong>
      <br>
      Runyu Ding*, <b>Jihan Yang*</b>, Xiaojuan Qi
      <br>
      <br>
      <em>European Conference on Computer Vision (ECCV), 2022.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/2204.01599">[PDF]</a>
          <a href="https://github.com/CVMI-Lab/DODA">[CODE]</a> <a href="https://github.com/CVMI-Lab/DODA"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/DODA?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>

<div class="publication">
  <img src="content/images/pcdet_min.png" class="publogo">
  <p>
      <strong>
          <a href="https://github.com/open-mmlab/OpenPCDet">OpenPCDet: An Open-source Toolbox for 3D Object Detection from Point Cloud
          </a>
      </strong>
      <br>
      OpenPCDet Development Team <strong style="color:#FF4500">(2nd Core Developer)</strong>
      <br>

      <span class="links">
        <a href="https://github.com/open-mmlab/OpenPCDet">[CODE]</a>
        <a href="https://github.com/open-mmlab/OpenPCDet"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/open-mmlab/OpenPCDet?style=social" /></a>
      </span>
  </p>
</div>


<div class="publication">
  <img src="content/images/dars_teaser.png" class="publogo" style="padding-bottom:20px;">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/2107.11279">Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation</a>
      </strong>
      <br>
      Ruifei He*, <b>Jihan Yang*</b>, Xiaojuan Qi
      <br>
      <br>
      <em>IEEE International Conference on Computer Vision (ICCV), 2021.</em>
      <span style="color: #FF4500;"><strong>[Oral]</strong></span>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/2107.11279">[PDF]</a>
          <a href="https://github.com/CVMI-Lab/DARS">[CODE]</a> <a href="https://github.com/CVMI-Lab/DARS"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/DARS?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>


<div class="publication">
  <img src="content/images/st3d_teaser.png" class="publogo" style="padding-bottom:20px;">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/2103.05346">ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection</a>
      </strong>
      <br>
      <b>Jihan Yang*</b>, Shaoshuai Shi*, Zhe Wang, Hongsheng Li, Xiaojuan Qi
      <br>
      <br>
      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/2103.05346">[PDF]</a>
          <a href="https://github.com/CVMI-Lab/ST3D">[CODE]</a> <a href="https://github.com/CVMI-Lab/ST3D"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CVMI-Lab/ST3D?style=social" /> </a>
      </span>
  </p>
</div>
<br>
<br>



<div class="publication">
  <img src="content/images/apoda_teaser.png" class="publogo" style="padding-bottom:20px;">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/1912.08954">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</a>
      </strong>
      <br>
      <b>Jihan Yang</b>, Ruijia Xu, Ruiyu Li, Xiaojuan Qi, Xiaoyong Shen, Guanbin Li, Liang Lin
      <br>
      <br>
      <em>AAAI, 2020.</em>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/1912.08954">[PDF]</a>
      </span>
  </p>
</div>
<br>
<br>


<div class="publication">
  <img src="content/images/afn_teaser.png" class="publogo" style="padding-bottom:20px;">
  <p> 
      <strong>
        <a href="https://arxiv.org/abs/1811.07456">Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation</a>
      </strong>
      <br>
      Ruijia Xu, Guanbin Li, <b>Jihan Yang</b>, Liang Lin
      <br>
      <br>
      <em>IEEE International Conference on Computer Vision (ICCV), 2019.</em><span style="color:#FF4500;"><strong>[Oral]</strong></span>
      <br>
      <span class="links">
          <a href="https://arxiv.org/abs/1811.07456">[PDF]</a>
          <a href="https://github.com/jihanyang/AFN">[CODE]</a> <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/jihanyang/AFN?style=social" />
      </span>
      <br>
      <br>
      <span style="color: #FF4500">Best Paper Award Nomination (one of the seven among 1,075 accepted papers) refer to <a href="https://iccv2019.thecvf.com/program/main_conference" style="">here.</a></span>
  </p>
</div>
<br>
<br>
<br>
<br>

<h2 id="experience" style="color:rgb(86, 1, 141)">Experience</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />

<ul style="font-size: 14px">
  <li style="margin-bottom: 10px">
    <span><strong>Courant Institute of Mathematical Sciences, NYU</strong></span>
    <div style="float: right;">April 2023 - Aug 2024</div>
    <br clear="all">
    <span>Research intern with <a href="https://www.sainingxie.com/">Prof. Saining Xie</a></span>
  </li>

  <li style="margin-bottom: 10px">
    <span><strong>Autonomous Driving Group, SenseTime</strong></span>
    <div style="float: right;">May 2020 - Oct 2020</div>
    <br clear="all"> 
    <span>Research intern with <a href="https://wang-zhe.me/">Dr. Zhe Wang</a></span>
  </li>

  <li style="margin-bottom: 10px">
    <span><strong>Youtu Lab, Tencent</strong></span>
    <div style="float: right;">Feb 2019 - Feb 2020</div>
    <br clear="all"> 
    <span>Research intern with <a href="https://www.linkedin.com/in/ruiyu-li-09664b134/?originalSubdomain=cn">Dr. Ruiyu Li</a> and <a href="https://scholar.google.com.hk/citations?user=PeMuphgAAAAJ&hl=en">Dr. Xiaoyong Shen</a></span>
  </li>

  <li style="margin-bottom: 10px">
    <span><strong>Research Group, YITU Technology</strong></span>
    <div style="float: right;">Jul 2018 - Sep 2019</div>
    <br clear="all"> 
    <span>Research intern</a></span>
  </li>

</ul>



<!-- * Bachelor, Aug 2013 - July 2017, Computer Science and Technology, Harbin Institute Technology. -->

<h2 id="honors--awards" style="color:rgb(86, 1, 141)">Honors &amp; Awards</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />

<table style="border-spacing:2px" width="100%">
    <tbody>
    <tr>
        <td>2nd place on 3D detection, 3D tracking and domain adaptation three tracks of Waymo Open Challenges</td>
        <td style="text-align: right">2020</td>
    </tr>
    <tr>
        <td>Postgraduate Scholarship, HKU</td>
        <td style="text-align: right">2020 - 2024</td>
    </tr>
    <tr>
        <td>Best Paper Nomination, IEEE International Conference on Computer Vision (0.2%)</td>
        <td style="text-align: right">2019</td>
    </tr>
    <tr>
        <td>Excellent Graduate Award of Sun Yat-sen University (2%)</td>
        <td style="text-align: right">2019</td>
    </tr>
    <tr>
        <td>Excellent Dissertations of Sun Yat-sen University (2%)</td>
        <td style="text-align: right">2019</td>
    </tr>
    <tr>
        <td>First Prize Scholarship of Sun Yat-sen University (4%)</td>
        <td style="text-align: right">2017, 2018</td>
    </tr>
   
</tbody></table>

<!-- ###################### -->

<h2 id="Academic_services" style="color:rgb(86, 1, 141)">Academic Services</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />
Conference Reviewer:
<ul>
  <li>CVPR: 21/22/23/24/25</li>
  <li>ICCV: 21/25</li>
  <li>ECCV: 22/24</li>
  <li>NeurIPS: 23/24</li>
  <li>ICLR: 24/25</li>
  <li>ICML: 25</li>
  <li>IROS: 23</li>
  <li>AAAI: 21</li>
  <li>MM: 24</li>
</ul> 
Journal Reviewer:
<ul>
  <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</li>
  <li>International Journal of Computer Vision (IJCV)</li>
</ul> 


<h2 id="Teaching" style="color:rgb(86, 1, 141)">Teaching</h2>
<hr style="margin-top:-16px;margin-bottom:10px;" />
<table id="Teaching" border="0" width="100%">
	<tbody>
		<tr>
			<td>ELEC3249 Pattern Recognition and Machine Intelligence</td><td></td><td style="text-align: right">2022-2023</td>
		</tr>
		<tr>
			<td>ENGG1310 Electricity and Electronics</td><td></td><td style="text-align: right">2021-2022</td>
		</tr>
		<tr>
			<td>ELEC3249 Pattern Recognition and Machine Intelligence</td><td></td><td style="text-align: right">2020-2021</td>
		</tr>
	</tbody>
</table>


<div width="80%" style="margin:10px auto;height: 150px; pointer-events: none;">
<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=dna1gh75RFxuLTLMZdN7u5SSyiCEmnOtvSjR75TKtTQ&cl=ffffff&w=a"></script> -->
<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=b8b8b8&w=300&t=n&d=dna1gh75RFxuLTLMZdN7u5SSyiCEmnOtvSjR75TKtTQ&co=ffffff&cmo=fc1a0b&cmn=21c983&ct=827d7d"></script> -->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=bsE8tO7zuYstQlZ4LK-y0ELcK4Uxw6_BtV4Opk5d6GY&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
<!-- <a href="https://clustrmaps.com/site/1apwn" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=59bdf5&w=300&t=tt&d=dna1gh75RFxuLTLMZdN7u5SSyiCEmnOtvSjR75TKtTQ&co=ffffff&ct=b8b3b3" /></a> -->
</div>
<p><br /></p>
<hr style="margin-top:80px;margin-bottom:10px;" />

    </div>
  </body>
</html>
